{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "171bfa44-9f6e-4637-ae7d-312303cb2c45",
   "metadata": {},
   "source": [
    "# üß† Training an MNIST Classifier from Scratch\n",
    "\n",
    "This project demonstrates how to implement and train a **fully connected neural network** from scratch using **NumPy** for the MNIST dataset. The code is structured to facilitate understanding of neural network basics, including forward propagation, backpropagation, and gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "- **Dataset**: MNIST (handwritten digits with 28x28 grayscale images)\n",
    "- **Frameworks Used**:\n",
    "  - **NumPy**: For mathematical operations (no high-level deep learning libraries)\n",
    "  - **PyTorch**: For data loading and dataset utilities\n",
    "  - **Matplotlib**: For visualizing training progress\n",
    "- **Network Architecture**:\n",
    "  - Input layer: 784 nodes (flattened 28x28 images)\n",
    "  - Hidden layer 1: 256 nodes, ReLU activation\n",
    "  - Hidden layer 2: 64 nodes, ReLU activation\n",
    "  - Output layer: 10 nodes (one for each digit class, 0-9), softmax activation\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Code Walkthrough\n",
    "\n",
    "### **1. Data Preparation**\n",
    "\n",
    "```python\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    transforms.Lambda(lambda x: x.view(-1))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='dataset', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='dataset', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "```\n",
    "\n",
    "- **Purpose**: Load and preprocess the MNIST dataset.\n",
    "- **Transformations**:\n",
    "  - Convert images to tensors.\n",
    "  - Normalize pixel values to have zero mean and unit variance.\n",
    "  - Flatten the 28x28 images into 1D vectors of size 784.\n",
    "- **DataLoader**: Used for batching and shuffling.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Neural Network Implementation**\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "```python\n",
    "class Scratch:\n",
    "    def __init__(self):\n",
    "        self.w1 = np.random.randn(256, 784) * np.sqrt(2 / 784)\n",
    "        self.b1 = np.zeros((1, 256))\n",
    "\n",
    "        self.w2 = np.random.randn(64, 256) * np.sqrt(2 / 256)\n",
    "        self.b2 = np.zeros((1, 64))\n",
    "\n",
    "        self.w3 = np.random.randn(10, 64) * np.sqrt(2 / 64)\n",
    "        self.b3 = np.zeros((1, 10))\n",
    "```\n",
    "\n",
    "- **Weights and Biases**:\n",
    "  - Initialized with **He initialization** to ensure stable gradients with ReLU.\n",
    "  - Biases are initialized to zeros.\n",
    "\n",
    "#### Forward Pass\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    self.x = x \n",
    "    self.z1 = x @ self.w1.T + self.b1\n",
    "    self.a1 = self.relu(self.z1)\n",
    "\n",
    "    self.z2 = self.a1 @ self.w2.T + self.b2\n",
    "    self.a2 = self.relu(self.z2)\n",
    "\n",
    "    self.z3 = self.a2 @ self.w3.T + self.b3\n",
    "    return self.z3\n",
    "```\n",
    "\n",
    "- **Layer Outputs**:\n",
    "  - `z1`, `z2`, `z3`: Linear transformations for each layer.\n",
    "  - `a1`, `a2`: Activations after applying ReLU.\n",
    "  - Final output (`z3`) is the logits (unscaled probabilities).\n",
    "\n",
    "#### Activation Functions\n",
    "\n",
    "```python\n",
    "@staticmethod\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "@staticmethod\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "```\n",
    "\n",
    "- **ReLU**: Used as the activation function for hidden layers.\n",
    "- **Softmax**: Used for converting logits into probabilities (defined separately).\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "```python\n",
    "@staticmethod\n",
    "def cross_entropy_loss(logits, labels):\n",
    "    logits_stable = logits - np.max(logits, axis=1, keepdims=True)\n",
    "    log_sum_exp = np.log(np.sum(np.exp(logits_stable), axis=1, keepdims=True))\n",
    "    loss_per_sample = -np.sum(labels * (logits_stable - log_sum_exp), axis=1)\n",
    "    return np.mean(loss_per_sample)\n",
    "```\n",
    "\n",
    "- **Purpose**: Compute cross-entropy loss for classification.\n",
    "- **Stability**: Uses log-sum-exp trick to prevent numerical instability.\n",
    "\n",
    "#### Backpropagation\n",
    "\n",
    "```python\n",
    "def backward(self, logits, labels, learning_rate=0.01):\n",
    "    N = labels.shape[0]\n",
    "    probs = self.softmax(logits)\n",
    "    dL_dz3 = (probs - labels) / N\n",
    "\n",
    "    dL_dw3 = dL_dz3.T @ self.a2\n",
    "    dL_db3 = np.sum(dL_dz3, axis=0, keepdims=True)\n",
    "\n",
    "    dL_da2 = dL_dz3 @ self.w3\n",
    "    dL_dz2 = dL_da2 * self.relu_derivative(self.z2)\n",
    "\n",
    "    dL_dw2 = dL_dz2.T @ self.a1\n",
    "    dL_db2 = np.sum(dL_dz2, axis=0, keepdims=True)\n",
    "\n",
    "    dL_da1 = dL_dz2 @ self.w2\n",
    "    dL_dz1 = dL_da1 * self.relu_derivative(self.z1)\n",
    "\n",
    "    dL_dw1 = dL_dz1.T @ self.x\n",
    "    dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True)\n",
    "\n",
    "    self.w3 -= learning_rate * dL_dw3\n",
    "    self.b3 -= learning_rate * dL_db3\n",
    "    self.w2 -= learning_rate * dL_dw2\n",
    "    self.b2 -= learning_rate * dL_db2\n",
    "    self.w1 -= learning_rate * dL_dw1\n",
    "    self.b1 -= learning_rate * dL_db1\n",
    "```\n",
    "\n",
    "- **Steps**:\n",
    "  - Compute gradients for weights and biases using the chain rule.\n",
    "  - Update weights and biases using gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Training Loop**\n",
    "\n",
    "```python\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.numpy()\n",
    "        labels_onehot = np.zeros((labels.shape[0], 10))\n",
    "        labels_onehot[np.arange(labels.shape[0]), labels.numpy()] = 1\n",
    "\n",
    "        logits = model.forward(images)\n",
    "        loss = model.cross_entropy_loss(logits, labels_onehot)\n",
    "        model.backward(logits, labels_onehot, learning_rate=LEARNING_RATE)\n",
    "\n",
    "        epoch_loss += loss * labels.shape[0]\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        correct += (preds == labels.numpy()).sum()\n",
    "        total += labels.shape[0]\n",
    "\n",
    "    avg_loss = epoch_loss / total\n",
    "    acc = correct / total\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(acc)\n",
    "```\n",
    "\n",
    "- **Steps**:\n",
    "  - Perform forward pass, compute loss, and backpropagate for each batch.\n",
    "  - Calculate epoch-level metrics (average loss and accuracy).\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Visualization**\n",
    "\n",
    "```python\n",
    "clear_output(wait=True)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label=\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- **Real-time Updates**:\n",
    "  - Plots the training loss and accuracy after each epoch.\n",
    "  - Loss and accuracy are displayed side-by-side for easy comparison.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Results\n",
    "\n",
    "- **Final Output**:\n",
    "  - Training loss and accuracy are printed after each epoch.\n",
    "  - Training progress is visualized with live plots.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Notes & Recommendations\n",
    "\n",
    "- **One-hot Encoding**: Labels are converted to one-hot vectors for cross-entropy.\n",
    "- **Numerical Stability**: Log-sum-exp trick ensures stable loss computation.\n",
    "- **Scalability**: This implementation works well for learning but is not optimized for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like any further improvements or enhancements!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
